可控性目标
- 数值准确
- 表达合规
- 输出格式（遵从显示规范）
- 图文一致
- 个性化表达
- 结论实用

核心问题
- 垂直领域量纲和通用基模存在语义 Gap 和量纲不对齐
- 领域知识的注入

Post train
- 金融知识注入
- 指令遵循增强
- 领域知识拆分学习（？
RL 训练范式
- 丰富的 MCP 服务
- 深度工具调用
- expert in the loop

数据生产（？
- 结构化知识提取和训练：权威数据 -> 原子知识 -> 结构化知识
- 基于自省和纠错的数据飞轮：问题 -> LLM -> 思考，答案 -> 提示 -> 思考，答案

通用基座指令遵循难题
- 随指令复杂度提升和长度提升衰减
- 否定指令难遵从
基于自我演练与执行反馈指令增强训练
- 自我演练：原子指令+组合方式进行指令和对应回复自举
- 执行反馈：基于函数和 prompt （LLM）内容理解双重校验
- 对比训练：准确指令-回复pair做正样本训练，错误pair做负样本训练

对话框架中加入深度工具调用
- Step1 理解策划：问题拆解，内置金融专家行为框架
- Step2 检索调度：用 NL2API 的方式检索提供丰富供给，例如资讯内容、金融数据、专业工具、金融观点等
- Step3 表达生成：内容（表达框架，风格）和形式（markdown，图文，视频）两方面
- Step4 校验：拦截自省+二次生成，RLHF

MCP 服务：金融数据库、内容知识库、行情数据库、客服知识库、投顾 API、图表卡片库、服务工具库

可控性生成
- SFT+DPO+RL训练范式
- SFT 阶段获取高质量冷启动样本，DPO 对齐专家偏好
- RL 阶段兼顾严谨性约束、图表格式约束、峰值表达要求
	- Policy Model 生成回复
	- Ref Model（Frozen）生成参考 Token 概率
	- 规则 & RM Model 评估回复质量
	- RL Optimizer（DAPO/GRPO）

RL 阶段： Expert in the loop 训练范式
- query-data engine
	- 定义对话数据范围
	- 标注/改写冷启动样本
- Reward Engine
	- pointwise-thinking-RFT: 问题-回答 -- 思考+评价 <-- 人工标注 对错。例如用在格式正确、底线约束上
		- 规则 reward ， LLM Eval，Reward Model
	- pairwise-thinking-RFT: 问题-回答A/B -- 思考+排序 <--- 人工标注 A>B/ B> A。例如用在峰值体验上（是否实用、论据丰富、图文排版偏感性的）
		- 对维度进行细拆，哪个维度上更好
		- 打分标准：基础分、增量分；分为几档打分
		-  LLM Eval，Reward Model
- Policy Engine：多目标评价体系

数据严谨性 reward 为例
- 如何识别 Answer 中的错误数值
	- reference + query + answer 让模型判断是否存在 数值无中生有、数值张冠李戴、事实张冠李戴 的问题 -- 准确率 70%- 80%
	- 挑战：reference 和 answer 长度；数值表达形式多样（文本、图表、图片）
	- 解决方案：先抽取 answer 中的六元组（主体、指标、数值、时间），再给模型判断（可以自己训模型，把抽取和判断训到一个模型里）
	- 细粒度数值严谨性判断：细粒度事实性对齐框架 Mask-DPO -- 针对部分信息做判断，mask 掉不一致的部分
